{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "383b2356-3581-4e11-9a39-c51e1f39e3e8",
   "metadata": {},
   "source": [
    "This script performs a fine tuning of BART to perform summaries.\n",
    "\n",
    "About the DataSet:\n",
    "\n",
    "DataSet consists of 295,174 news articles scrapped from a Mexican Newspaper, along with its summary. Summaries were created using `StableBeluga-7B`. I left the LLM running for several days (weeks) in order to get all the summaries. The teacher observations are used for BART fine tunning.\n",
    "\n",
    "The objective for this is to have a lightweight model that can perform summarization as good as `StableBeluga-7B`, much faster and with much less computing resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa34f8cf-4c52-4329-b190-a31b45b50923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d6ee25b-94fa-4341-ab1a-e92f48dbed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "708885ca-1a3b-4ac2-861e-f90cb6b88c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "405a9cb20e0742f9a21c8ede39e7561e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.02G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\jordi\\venvs\\bart\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\Users\\jordi\\.cache\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BART model and tokenizer\n",
    "model_name = \"facebook/bart-large\"  # or another BART variant\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f6aca04-ebdc-4035-a746-95cdbc49c4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the StableBeluga summaries data set that was pre-processed in 1-make_dataset notebook\n",
    "\n",
    "with open('datasets/BART_data_set.json', \"r\") as f:\n",
    "    train_set = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea7f445d-c411-4fd1-9e8f-e961018591a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "summs_df = pd.DataFrame.from_dict(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34235a0c-0305-4945-a399-327dfbd7c78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in train and validation\n",
    "train_df, val_df = train_test_split(summs_df, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "560098bf-c983-42a4-9c12-010d781b472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train and validation data\n",
    "val_df.to_json(\"datasets/BART_validation_data.json\", orient='records')\n",
    "train_df.to_json(\"datasets/BART_train_data.json\", orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9278b50-1037-49ce-8307-71c23a283cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class NewsSummaryDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=1024, padding='max_length'):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_len = max_len\n",
    "        self.padding = padding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # article is the input\n",
    "        article = self.data.iloc[idx]['article']\n",
    "        # summary will be the target label\n",
    "        summary = self.data.iloc[idx]['summary']\n",
    "        inputs = self.tokenizer(\n",
    "            article, \n",
    "            max_length=self.max_len, \n",
    "            truncation=True, \n",
    "            padding=self.padding,  # Enable padding\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        targets = self.tokenizer(\n",
    "            summary, \n",
    "            max_length=self.max_len, \n",
    "            truncation=True, \n",
    "            padding=self.padding,  # Enable padding\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return inputs.input_ids.squeeze(), targets.input_ids.squeeze()\n",
    "\n",
    "# Prepare the dataset and data loader\n",
    "train_dataset = NewsSummaryDataset(train_df, tokenizer)\n",
    "val_dataset = NewsSummaryDataset(val_df, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e0e45d8-98ab-4f6d-97c7-2d4d20c52d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr = 2e-5, # args.learning_rate\n",
    "    eps = 1e-8 # args.adam_epsilon\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f110ae9-2b01-402f-b9dd-da43ee741bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 3\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_loader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps = 0, # Default value in run_glue.py\n",
    "    num_training_steps = total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c91a7c4-7213-4715-99fd-f7d11ee42efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Step 125439, Loss: 0.037962757050991064\r"
     ]
    }
   ],
   "source": [
    "# Define the number of steps to accumulate gradients\n",
    "accumulation_steps = 16  # Adjust this according to your needs\n",
    "\n",
    "# Put the model into training mode. Don't be mislead--the call to \n",
    "# `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "# `dropout` and `batchnorm` layers behave differently during training\n",
    "# vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "model.train()\n",
    "for epoch in range(epochs):  # number of epochs\n",
    "    # Clear the gradients\n",
    "    model.zero_grad()\n",
    "    # empty gpu cache to free memory\n",
    "    torch.cuda.empty_cache()\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        \n",
    "        articles, summaries = batch\n",
    "        articles = articles.to(device)\n",
    "        summaries = summaries.to(device)\n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # This will return the loss (rather than the model output) because we\n",
    "        # have provided the `labels`.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        outputs = model(input_ids=articles, labels=summaries)\n",
    "        # Update loss\n",
    "        loss = outputs.loss / accumulation_steps  # Normalize the loss\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            # Clip the norm of the gradients to 1.0 to prevent the \"exploding gradients\" problem\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            # Update the learning rate.\n",
    "            scheduler.step() # Uncomment if using a learning rate scheduler\n",
    "            # Clear the gradients\n",
    "            model.zero_grad()\n",
    "            print(f\"Epoch {epoch}, Step {step}, Loss: {loss.item() * accumulation_steps}\", end='\\r')  # Un-normalize the loss for reporting\n",
    "            \n",
    "\n",
    "    # Perform an optimization step if there are any unprocessed gradients remaining after the last batch\n",
    "    if len(train_loader) % accumulation_steps != 0:\n",
    "        # Clip the norm of the gradients to 1.0 to prevent the \"exploding gradients\" problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        # Update the learning rate.\n",
    "        scheduler.step() # Uncomment if using a learning rate scheduler\n",
    "        # Clear the gradients\n",
    "        model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "889cf4c2-3bd6-49e5-bcf4-fa4b504f65f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model_save/bart_summarizer\\\\tokenizer_config.json',\n",
       " './model_save/bart_summarizer\\\\special_tokens_map.json',\n",
       " './model_save/bart_summarizer\\\\vocab.json',\n",
       " './model_save/bart_summarizer\\\\merges.txt',\n",
       " './model_save/bart_summarizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save_pretrained(\"./model_save/bart_summarizer\")\n",
    "tokenizer.save_pretrained(\"./model_save/bart_summarizer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
