{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0ff6a63-8a6e-4ad2-9558-d8c168b70cf5",
   "metadata": {},
   "source": [
    "In this script we feed the news articles to `StableBeluge-7B` LLM in order to get the teacher summary observations that will be used throught the training phase of the lightweight BART model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aadb102b-348a-4cbf-88b7-f930edd09eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements\n",
    "\n",
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "#!pip install transformers -q\n",
    "#!pip install sentence_transformers -q\n",
    "#!pip install nltk -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdb43c59-1010-4324-b0ac-bccffbb52240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import json\n",
    "from time import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a971a468-cc9f-4b71-ae99-9a5dab4884e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f866c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a3c18c0-16d4-40ec-8bfb-cde19d7635f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices  2\n",
      "Device 0: NVIDIA GeForce RTX 4090\n",
      "Device 1: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "print(\"Available devices \", torch.cuda.device_count())\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f'Device {i}:', torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6291ee66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu118\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0995f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4eb17f3b434e94ab6357d1d17550bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"stabilityai/StableBeluga-7B\", \n",
    "    use_fast=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"stabilityai/StableBeluga-7B\", \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    #low_cpu_mem_usage=True, \n",
    "    #device_map=\"auto\"\n",
    ")#.to(\"cuda\")\n",
    "model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ef62ffb-19d0-4a71-a4d6-a63775bd82c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\").to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f641a30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "101973cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYS_PROMPT = \"### System:\\nYou are StableBeluga, an AI that follows instructions extremely well. Help as much as you can. Remember, be safe, and don't do anything illegal.\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4d0209e-8f47-4a84-b656-bbb21194b1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress_bar(iteration, total, bar_length=50):\n",
    "    progress = float(iteration) / float(total)\n",
    "    arrow = '=' * int(round(progress * bar_length) - 1)\n",
    "    spaces = ' ' * (bar_length - len(arrow))\n",
    "\n",
    "    print(f'Progress: [{arrow + spaces}] {int(progress * 100)}%', end='\\r')\n",
    "    \n",
    "def get_most_relevant_sentences(article_text, embeddings_model, top_n_perc=.5):\n",
    "    \"\"\"\n",
    "    Extract only the \"top_n_perc\" sentences of a text \"article_text\".\n",
    "    Top n sentences are considered the sentences that are most similar to the whole text.\n",
    "    It is kind of a simple extractive summarization.\n",
    "    Steps:\n",
    "     1. The embedding of the whole text is computed. \n",
    "     2. The whole text is broken into individual sentences.\n",
    "     3 Embeddings of each individual sentence are computed.\n",
    "     4. Cosine similarity of each individual sentences against the embedding of the whole text is computed.\n",
    "     5. Get top n sentences (sentences that more closely resembele the idea of the whole text).\n",
    "    Receives:\n",
    "     - article_text: str: The text of a news article.\n",
    "     - embeddings_model: object: The model that will be used for embeddings. Model should have a .encode functionality to compute the embeddings.\n",
    "    Returns:\n",
    "     - Text containing only the top n % most representative sentences of a text.\n",
    "    \"\"\"\n",
    "    # compute embedding of the whole text\n",
    "    whole_text_embedding = embeddings_model.encode(article_text, show_progress_bar=False)\n",
    "    # break text in sentences\n",
    "    sentences = nltk.sent_tokenize(article_text)\n",
    "    # store the sentences in a DataFrame\n",
    "    sentences_df = pd.DataFrame(sentences, columns=['sentence'])\n",
    "    # compute embeddings of each sentence individually\n",
    "    sentences_embeddings = embeddings_model.encode(sentences, show_progress_bar=False)\n",
    "     \n",
    "    # compute cosine similarities of the whole text vs each individual sentence\n",
    "    cosine_sims = cosine_similarity(\n",
    "        whole_text_embedding.reshape(1, -1), \n",
    "        sentences_embeddings\n",
    "    )\n",
    "    # store cosine similarities on a column of the DataFrame\n",
    "    sentences_df['similarity'] = cosine_sims[0]\n",
    "    sentences_df.reset_index(inplace=True)\n",
    "    # n sentences tied to top_n_perc of the article\n",
    "    top_n = round(len(sentences)*top_n_perc)\n",
    "    # Top n percent sentences that capture the main idea, sorted by how they appear in the text\n",
    "    most_relevant_sentences = sentences_df.sort_values(\n",
    "        # sort by similarity\n",
    "        by='similarity', \n",
    "        # most similars at the top\n",
    "        ascending=False\n",
    "    ).head(\n",
    "        # top 20%\n",
    "        top_n\n",
    "    ).sort_values(\n",
    "        # sort them back by how they appear in the original text\n",
    "        by='index'\n",
    "    )[\n",
    "        # get senteces\n",
    "        'sentence'\n",
    "    ].values.tolist() # to python list\n",
    "    return ' '.join(most_relevant_sentences)\n",
    "\n",
    "def summarize_article(article_text, model, tokenizer, top_n_perc = .2):\n",
    "    \"\"\"\n",
    "    Summarize a single article text by using Stable Beluga\n",
    "    \"\"\"\n",
    "    start = time()\n",
    "    message = \"\"\"\n",
    "    \n",
    "    Please summarize this:\n",
    "    \n",
    "    {0}\n",
    "    \"\"\".format(article_text)\n",
    "    prompt = f\"{SYS_PROMPT}### User: {message}\\n\\n### Assistant:\\n\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "    token_length = inputs[\"input_ids\"].shape[1]\n",
    "    if token_length > 4096:\n",
    "        #print(\"token lengths were greater than 4096, summarizing the most relevant sentences\")\n",
    "        # summarize only the most importante sentences of the article\n",
    "        most_relevant_pieces = get_most_relevant_sentences(\n",
    "            article_text, \n",
    "            embedding_model, \n",
    "            top_n_perc = top_n_perc\n",
    "        )\n",
    "        summary = summarize_article(most_relevant_pieces, model, tokenizer, top_n_perc = .1)\n",
    "        return summary\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, do_sample=False, top_p=0.95, top_k=0, max_new_tokens=150)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "    end = time()\n",
    "    #print(end-start)\n",
    "    return response, end-start\n",
    "\n",
    "def summarize_news_articles(news_articles_to_summarize, already_summarized_news_articles, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Trigger the batch summarization process for all news articles\n",
    "    \"\"\"\n",
    "    total_len = len(news_articles_to_summarize)\n",
    "    for i, art in enumerate(news_articles_to_summarize):\n",
    "        this_art = deepcopy(art)\n",
    "        output, time_taken = summarize_article(\n",
    "            this_art.get('content_en'), \n",
    "            model, \n",
    "            tokenizer\n",
    "        )\n",
    "        assitant_ix = output.find(\"### Assistant:\")\n",
    "        summary_ix = assitant_ix+len(\"### Assistant:\\n \")\n",
    "    \n",
    "        summary = output[summary_ix:]\n",
    "        this_art['summary'] = summary\n",
    "        already_summarized_news_articles += [this_art]\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            with open('datasets/summarized_news.json', 'w') as f:\n",
    "                json.dump(already_summarized_news_articles, f)\n",
    "\n",
    "        if i%10 == 0:\n",
    "            print_progress_bar(iteration=i, total=total_len)\n",
    "            torch.cuda.empty_cache()\n",
    "    with open('datasets/summarized_news.json', 'w') as f:\n",
    "        json.dump(already_summarized_news_articles, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85f200d1-5e60-4ed0-a6e7-4c908efea3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the translated news articles\n",
    "path_file = 'datasets/translated_news.json'\n",
    "with open(path_file, 'r') as jfile:\n",
    "    news_articles = json.load(jfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64410a32-7623-483a-8b52-c9fe90cfb28b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sofar summarized news\n",
    "path_file = 'datasets/summarized_news.json'\n",
    "with open(path_file, 'r') as jfile:\n",
    "    summarized_news_articles = json.load(jfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a634395-009c-4d3f-8018-f1caccb377c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_df = pd.DataFrame.from_dict(summarized_news_articles)\n",
    "\n",
    "already_summarized = set(summaries_df['h1'])\n",
    "\n",
    "all_news_df = pd.DataFrame.from_dict(news_articles)\n",
    "\n",
    "all_news_titles = set(all_news_df['h1'])\n",
    "\n",
    "pending_summary = all_news_titles - already_summarized\n",
    "\n",
    "articles_pending_summary_df = all_news_df[all_news_df['h1'].isin(pending_summary)]\n",
    "\n",
    "articles_pending_summary = articles_pending_summary_df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b68a511-288f-4015-86aa-b9b8a1d2638d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286120"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(already_summarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4296e1f2-fba6-4120-a8ce-80f906748bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6305"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles_pending_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f177c828-c403-4338-bc4c-4e8bb5399c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9784389159613576"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(already_summarized)/(len(already_summarized)+len(articles_pending_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f603543-adff-429e-b60f-d07736a26c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [===                                               ] 7%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [==========================================        ] 86%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10985 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [================================================= ] 99%\r"
     ]
    }
   ],
   "source": [
    "summarize_news_articles(\n",
    "    news_articles_to_summarize=articles_pending_summary, \n",
    "    already_summarized_news_articles=summarized_news_articles,\n",
    "    model=model, \n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "649d51c2-7f04-48a9-804a-445f4882a4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "295174"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summarized_news_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a676373-7aa8-4da6-96ba-5980ebfca103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h1': 'Revocación de mandato es posible gracias al INE, presume Lorenzo Córdova\\xa0',\n",
       " 'h2': 'El consejero presidente del INE, también desmintió que el Instituto se niegue a promover la consulta así como debates sobre las distintas posturas sobre el tema\\xa0\\r\\n',\n",
       " 'h3': 'Más Información',\n",
       " 'date': '27/02/2022 16:58',\n",
       " 'author': 'Redacción ',\n",
       " 'content': 'Tras rechazar que el Instituto Nacional Electoral ( INE ) esté en contra del proceso de revocación de mandato , su presidente, Lorenzo Córdova Vianello, aseguró que este inédito ejercicio de democracia participativa es posible gracias al órgano electoral. “Es falso que el INE pretenda obstaculizar la revocación de mandato; todo lo contrario: si la revocación de mandato va es gracias al INE y a las miles de personas que están siendo capacitadas para instalar las casillas, recibir y contar el voto de sus vecinos y vecinas el próximo 10 de abril”, subrayó en un video publicado en las redes sociales. Lorenzo Córdova recordó que es una facultad constitucional del INE y atribución exclusiva, la promoción de este proceso, “lo que ha venido ocurriendo desde que se emitió la convocatoria respectiva el pasado 4 de febrero”. En este sentido, garantizó que el INE, de la mano de la ciudadanía, seguirá adelante con la organización de este proceso, dando todos los pasos necesarios para garantizar el derecho a participar  en la jornada del próximo 10 de abril a toda la ciudadanía. Por otra parte desmintió que el INE se niegue a promover la realización de la consulta de revocación de mandato y debates sobre las distintas posturas sobre el tema. Recalcó que el Consejo General del INE aprobó la realización de tres foros nacionales de discusión sobre la revocación de mandato del Presidente de la República. “En apego a la Ley Federal en la materia, que ordena que el INE debe realizar al menos dos foros públicos , se determinó llevar a cabo tres encuentros de este tipo para promover la participación de la ciudadanía en el proceso”. El consejero presidente del INE detalló que estos foros nacionales se llevarán a cabo en la Ciudad de México los días 25 y 31 de marzo y 3 de abril, y adicionalmente se realizarán ejercicios similares en las 32 entidades, e incluso podrán celebrarse a nivel distrital. Lee también:\\xa0Aprueba INE multa de 689.1 mdp a partidos Informó que en los foros participarán personas que han expresado públicamente su opinión en torno a este proceso, “para dialogar sobre las dos posturas posibles previstas en la ley : por un lado, que se le revoque el mandato al titular del Poder Ejecutivo por pérdida de confianza , o bien por el otro, que siga en la Presidencia de la República hasta el término de su mandato. Destacó que los encuentros serán conducidos por la coordinación de comunicación social del INE, “con absoluta imparcialidad  y objetividad , como suele caracterizarse su trabajo, garantizando así la equidad entre los posicionamientos de los participantes”. Córdova Vianello puntualizó que el objetivo es promover la discusión de las diversas posturas en torno a la revocación de mandato , a fin de que la ciudadanía tenga más elementos para decidir su participación “en este inédito proceso de democracia directa”. Lee también:\"Respeten las reglas del juego, que el INE no fue quien las fijó\": Córdova sobre revocación de mandato ardm/rmlgv',\n",
       " 'h1_en': 'Revocation of mandate is possible thanks to the INE, presumed Lorenzo Córdova',\n",
       " 'h2_en': 'The chair of the INE, also denied that the Institute refused to promote consultation as well as debates on the different positions on the subject.',\n",
       " 'content_en': 'After rejecting that the National Electoral Institute (INE) is against the process of revocation of the mandate, its president, Lorenzo Córdova Vianello, assured that this unprecedented exercise of participatory democracy is possible thanks to the electoral body. “It is false that the INE intends to hinder the revocation of the mandate; quite the contrary: if the revocation of the mandate goes thanks to the INE and to the thousands of people who are being trained to install the cells, receive and count the vote of their neighbors on April 10,” he stressed in a video published on social networks. Lorenzo Córdova recalled that it is a constitutional faculty of the INE and exclusive attribution, the promotion of this process, “what has been happening since the respective call was issued on February 4”. In this sense, he guaranteed that the INE, with the help of the citizenry, will continue to organize this process, taking all the necessary steps to guarantee the right to participate in the day of next April 10 to all the citizenry. On the other hand, he denied that the INE refused to promote the holding of the consultation to revoke the mandate and debates on the different positions on the subject. He stressed that the General Council of the INE approved the holding of three national forums for discussion on the revocation of the mandate of the President of the Republic. “In accordance with the Federal Law on the subject, which orders that the INE should hold at least two public forums, it was determined to hold three meetings of this kind to promote the participation of the citizenry in the process.” The president of the INE explained that these national forums will be held in Mexico City on 25 and 31 March and 3 April, and in addition similar exercises will be held in all 32 entities, and may even be held at district level. Also read: INE approves 689.1 mdp fine to parties He reported that in the forums will participate people who have publicly expressed their opinion about this process, \"to dialogue on the two possible positions provided for in the law : on the one hand, that the mandate is revoked to the holder of the executive branch for loss of confidence, or on the other, that he remains in the Presidency of the Republic until the end of his term of office. He stressed that the meetings will be led by the coordination of social communication of the INE, \"with absolute impartiality and objectivity, as their work is usually characterized, thus guaranteeing equity among the positions of the participants\". Córdova Vianello pointed out that the aim is to promote the discussion of the various positions regarding the revocation of the mandate, so that citizens have more elements to decide their participation “in this unprecedented process of direct democracy”. Also read:\"Respect the rules of the game, that the INE was not the one who set them\": Cordova on revocation of command ardm/rmlgv',\n",
       " 'content_len': 2980.0,\n",
       " 'summary': 'The National Electoral Institute (INE) has denied claims that it is against the process of revoking the mandate. INE President Lorenzo Córdova Vianello stated that the INE is promoting this unprecedented exercise of participatory democracy and will continue to organize the process, taking necessary steps to guarantee the right to participate on April 10. The INE will hold three national forums for discussion on the revocation of the mandate, with the aim of promoting the discussion of various positions regarding the revocation of the mandate, so that citizens have more elements to decide their participation in this process.</s>'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarized_news_articles[145326]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
