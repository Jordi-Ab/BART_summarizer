{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f63acc63-4f74-45ac-b2d5-a3a69e039164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from time import time\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class BARTSummarizer():\n",
    "    \"\"\"\n",
    "    BART Summarizer model, fine tuned with \"El Universal\" news.\n",
    "    True label summaries were generated with StableBeluga7B LLM.\n",
    "    \"\"\"\n",
    "    def __init__(self, path_to_model = \"./model_save/\"):\n",
    "        \"\"\"\n",
    "        Loads the BART pretrained model\n",
    "        \"\"\"\n",
    "        # Load pre-trained BART model and tokenizer\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model_name = \"{0}bart_summarizer\".format(path_to_model) \n",
    "        self.model = BartForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
    "        self.tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "        self.embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\").to(self.device)\n",
    "        \n",
    "    def get_most_relevant_sentences(article_text, embeddings_model, top_n_perc=.5):\n",
    "        \"\"\"\n",
    "        Extract only the \"top_n_perc\" sentences of a text \"article_text\".\n",
    "        Top n sentences are considered the sentences that are most similar to the whole text.\n",
    "        It is kind of a simple extractive summarization.\n",
    "        Steps:\n",
    "         1. The embedding of the whole text is computed. \n",
    "         2. The whole text is broken into individual sentences.\n",
    "         3 Embeddings of each individual sentence are computed.\n",
    "         4. Cosine similarity of each individual sentences against the embedding of the whole text is computed.\n",
    "         5. Get top n sentences (sentences that more closely resembele the idea of the whole text).\n",
    "        Receives:\n",
    "         - article_text: str: The text of a news article.\n",
    "         - embeddings_model: object: The model that will be used for embeddings. Model should have a .encode functionality to compute the embeddings.\n",
    "        Returns:\n",
    "         - Text containing only the top n % most representative sentences of a text.\n",
    "        \"\"\"\n",
    "        # compute embedding of the whole text\n",
    "        whole_text_embedding = embeddings_model.encode(article_text, show_progress_bar=False)\n",
    "        # break text in sentences\n",
    "        sentences = nltk.sent_tokenize(article_text)\n",
    "        # store the sentences in a DataFrame\n",
    "        sentences_df = pd.DataFrame(sentences, columns=['sentence'])\n",
    "        # compute embeddings of each sentence individually\n",
    "        sentences_embeddings = embeddings_model.encode(sentences, show_progress_bar=False)\n",
    "         \n",
    "        # compute cosine similarities of the whole text vs each individual sentence\n",
    "        cosine_sims = cosine_similarity(\n",
    "            whole_text_embedding.reshape(1, -1), \n",
    "            sentences_embeddings\n",
    "        )\n",
    "        # store cosine similarities on a column of the DataFrame\n",
    "        sentences_df['similarity'] = cosine_sims[0]\n",
    "        sentences_df.reset_index(inplace=True)\n",
    "        # n sentences tied to top_n_perc of the article\n",
    "        top_n = round(len(sentences)*top_n_perc)\n",
    "        # Top n percent sentences that capture the main idea, sorted by how they appear in the text\n",
    "        most_relevant_sentences = sentences_df.sort_values(\n",
    "            # sort by similarity\n",
    "            by='similarity', \n",
    "            # most similars at the top\n",
    "            ascending=False\n",
    "        ).head(\n",
    "            # top 20%\n",
    "            top_n\n",
    "        ).sort_values(\n",
    "            # sort them back by how they appear in the original text\n",
    "            by='index'\n",
    "        )[\n",
    "            # get senteces\n",
    "            'sentence'\n",
    "        ].values.tolist() # to python list\n",
    "        return ' '.join(most_relevant_sentences)\n",
    "\n",
    "    def _tokenize_article(self, article_text):\n",
    "        \"\"\"\n",
    "        Tokenize an article text. If tokenized articles is greater than 1024 tokens,\n",
    "        shorten the text, keep shortening until text no longer exceeds 1024 tokens\n",
    "        \"\"\"\n",
    "        perc_to_keep = 0.9\n",
    "        while True:\n",
    "            inputs = self.tokenizer(article_text, return_tensors='pt')\n",
    "            if inputs['input_ids'].shape[1]>1024:\n",
    "                # tokenized article exceeded 1024 tokens\n",
    "                # shorten the text\n",
    "                article_text = get_most_relevant_sentences(\n",
    "                    article_text, \n",
    "                    top_n_perc = perc_to_keep\n",
    "                )\n",
    "                perc_to_keep -= 0.1\n",
    "                # keep iterating\n",
    "            else:\n",
    "                # text no longer exceed 1024 tokens\n",
    "                break\n",
    "        return inputs['input_ids'].to(self.device)\n",
    "        \n",
    "    def summarize(self, article_txt):\n",
    "        \"\"\"\n",
    "        Perform the summarization task\n",
    "        \"\"\"\n",
    "        inputs = self._tokenize_article(article_txt)\n",
    "        with torch.no_grad():\n",
    "            summary_ids = self.model.generate(\n",
    "                inputs, \n",
    "                num_beams=4, \n",
    "                max_length=250, \n",
    "                early_stopping=True\n",
    "            )\n",
    "        summary = self.tokenizer.decode(\n",
    "            summary_ids[0], \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4c4752d-eb8c-46f3-bc90-414590afd1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the StableBeluga summaries validation data set\n",
    "with open('datasets/BART_validation_data.json', \"r\") as f:\n",
    "    validation_set = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb784a55-a942-4504-878c-459a0178f52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = BARTSummarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41136352-e2ca-4e43-8cbe-c6fd7c60ac45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News Article:\n",
      "The divorce between Alfredo Adame and Mary Paz Banquells, more than bad things, seems to have brought prosperity to the actress's house and her three children, because after the separation and the arrival of the pandemic, they found themselves in a compromised economic situation, which led her and them to start businesses that started being home and today, almost three years later, have become a success. In their visit to “Winner”, Mary Paz Banquells and her eldest son, Diego Adame, spoke of the dessert business they have undertaken as a family, after the pandemic by Covid-19 forced them to put a pause in their business, in which they hosted puppies from families who went on a trip and left their pets to their care. Diego recalled that, from one day to the next, the profits generated by babysitting doggies were frozen, because people were confined, so they could not leave the house and, therefore, they did not need to take care of their doggies, on the contrary, they could spend with them all the time, regardless of whether they were on a work meeting or during a class, since these occupations were carried out virtually. Read also: Millie Bobby Brown is accused of kissing an actor without her consent It was so that one day she realized that her mother was distressed by the economic situation they could face if the confinements were not lifted in the near future, so Diego sat down to talk to her, because because of her university studies related to the business branch, she told her that the best thing they could do to solve the problem was to start a business from home. In that way,\" Banquells explained, \"he asked her what she was good at, and she replied that making desserts, especially the orange thread, and from there, the Adame Banquells started the business that, still today, has given them many fruits; while Diego was in charge of spreading the business on social networks, his other two children helped her with cooking and distributing orders. Photo: Instagram Over time, Banquells added new recipes to offer a variety of desserts, which worked very well. Later, when the Covid-19 cases decreased and the activities began to normalize, the lodging of doggies in her house was activated again, so the actress had to learn to combine one project with another; in the mornings she took out the puppies and in the evenings prepared the desserts. But this whole process was an effort together, as his son Diego called his family a four-man boat, where they all sail together. This union, Mary Paz trusted, was born since her children were very young, because when she reflected that she had only sons, she became afraid that they would eventually turn away from her because they did not share tastes, so she began to take interest in the interests of the three and appropriate them, which made her accompany them to comic book conventions and disguise themselves. In addition to running the businesses he has undertaken, Banquells will appear in a work produced by Silvia Pasquel, which will be released early next year. Read also: Henry Cavill outnumbers BTS member and crowns himself as the handsomest man of 2022 Subscribe here to receive directly in your mail our newsletters about news of the day, opinion, weekend plans. melc\n",
      "----------------\n",
      "BART Summary:\n",
      "The divorce between Alfredo Adame and Mary Paz Banquells has brought prosperity to the actress and her three children. After the pandemic, they found themselves in a compromised economic situation, leading them to start businesses from home. The dessert business they have undertaken as a family has become a success, as they hosted puppies from families who went on a trip and left their pets to their care. The family has added new recipes to offer a variety of desserts, which worked well. When the Covid-19 cases decreased, the lodging of doggies in her house was activated again, so the actress had to learn to combine one project with another.\n",
      "----------------\n",
      "StableBeluga Summary:\n",
      "After her divorce from Alfredo Adame, Mary Paz Banquells and her three children faced financial difficulties due to the pandemic. To overcome this, they started a dessert business from home, which has become a success. Banquells' son, Diego Adame, helped her with social media marketing, while her other two children assisted with cooking and delivering orders. The business has expanded to offer a variety of desserts, and when the pandemic subsided, they resumed hosting puppies in their home. Banquells will also appear in a work produced by Silvia Pasquel, which will be released early next year.\n"
     ]
    }
   ],
   "source": [
    "ix = 0\n",
    "art = validation_set[ix]['article']\n",
    "sb_summ = validation_set[ix]['summary']\n",
    "summary = summarizer.summarize(art)\n",
    "\n",
    "print(\"News Article:\")\n",
    "print(art)\n",
    "print('----------------')\n",
    "print(\"BART Summary:\")\n",
    "print(summary)\n",
    "print('----------------')\n",
    "print(\"StableBeluga Summary:\")\n",
    "print(sb_summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38b85cb5-075c-48e1-aecb-127a6c48ec63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News Article:\n",
      "portfolio@eluniversal.com.mx The governor of the Banco de México (Banxico), Alejandro Díaz de León, said that the national economy has stagnated in the face of an environment of internal uncertainty and external factors that pressure the performance of the country. “The average of the last four to five quarters has a very low growth, we could talk about a certain stagnation in the economic activity,” he said. By participating in the Banorte Strategy Forum 2019, Díaz de León stressed that among the elements that have led to this condition of the Mexican economy is the contraction in the industrial production, as well as a slowdown in the services. “We are facing an environment of lower internal economic slowdown than what was anticipated and the external environment has not helped.” In the opinion of the official, weaknesses in governance prevail in the country that affect the growth, such is the use of insecurity and the strengthening of the rule of law. He added that the lower economic growth environment has led to more than the productive sectors working below its potential. On the reduction of 25 points base in its reference rate, the governor of Banxicón explained that there have been improvements in the behavior of inflation, which is now, which is likely to be considered to be less likely to be affected by the economic growth, which is less.\n",
      "----------------\n",
      "BART Summary:\n",
      "The governor of the Banco de México (Banxico), Alejandro Díaz de León, stated that the national economy has stagnated due to internal uncertainty and external factors that pressure the performance of the country. He mentioned that the average of the last four to five quarters has very low growth, leading to stagnation in economic activity. Among the factors contributing to this condition is a contraction in industrial production and a slowdown in services. The lower economic growth environment has led to more than the productive sectors working below their potential.\n",
      "----------------\n",
      "StableBeluga Summary:\n",
      "The governor of Banco de México (Banxico), Alejandro Díaz de León, stated that the Mexican economy has stagnated due to internal uncertainty and external factors. He mentioned that the country's economic growth has been low for the last four to five quarters, leading to a stagnation in economic activity. Díaz de León attributed this condition to a contraction in industrial production and a slowdown in services. He also mentioned that weak governance and insecurity have affected the country's growth. The governor stated that the lower economic growth environment has led to productive sectors working below their potential.\n"
     ]
    }
   ],
   "source": [
    "ix = 1\n",
    "art = validation_set[ix]['article']\n",
    "sb_summ = validation_set[ix]['summary']\n",
    "summary = summarizer.summarize(art)\n",
    "\n",
    "print(\"News Article:\")\n",
    "print(art)\n",
    "print('----------------')\n",
    "print(\"BART Summary:\")\n",
    "print(summary)\n",
    "print('----------------')\n",
    "print(\"StableBeluga Summary:\")\n",
    "print(sb_summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81597d15-81d6-499f-a882-ea5c7f2090b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44277"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d15a19ae-58e6-4e9b-9bc8-8dfa3d40871c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News Article:\n",
      "Zacatecas.- The state and federal electoral authorities in Zacatecas confirmed that the basic box 667 could not be installed in the community of Guadalupe Victoria in the municipality of Jerez, because the cell officials were threatened with death, therefore, the non-installation of that box was determined for the safety of the electors and officials. This confirmation was announced in the first incident court issued in the sessions of the electoral councils of the National Electoral Institute (INE) and the Electoral Institute of the State of Zacatecas (IEEZ), so it was determined that the electoral packages of this cell will be returned intact to the facilities of the district and municipal councils. During the session of the General Council of the IEEZ held at 11:00 a.m. for the first cut of incidents, it was announced that in the Incident System of the Election Day (SIJE) was confirmed the non-final installation of a “destination box” in the municipality of Jerez, which, grouped to four more squares than previously the INE decided to relocate them for security issues. That is, previously, the INE authorities, due to a climate of insecurity and violence in certain areas in that municipality, decided to relocate cella 665, 673, 666 and 703 located in the communities of Villa Hermosa (132 voters), Monte de los García (151 voters), Cieneguita de Fernández (155 voters) and Ordoñez (106 voters), which were sent to cella 667 of the community of Guadalupe Victoria 667 (139 voters). Read also: With delays of up to four hours, Sonora squares start voting day However, due to this same situation of risk or insecurity this morning in the SIJE it was reported that both the president of the square and the alternate received death threats and it was determined not to install that box. It should be mentioned that previously, the INE authorities had recognized the relocation of several squares in the municipalities of Jerez, Tepetongo and Monte Escobedo, in addition the Ministry of Public Security of the State has located this region adjacent to municipalities of Jalisco as a red focus for the presence and struggle of organized crime groups. In the second cut of incidents, the Information System of the Election Day (SIJE) reported until 13:52 the report of 2,512 cells installed in the state and the final report of 64 were missing, because the computation system that feeds this system has presented intermittances in reports that have been slow and in some cases reports are being sent via voice. In this second court, 54 incidents were recorded, of which 31 were already resolved. Within the complaints submitted by the political parties, they are concerned with the nonconformity of the dissemination of the results of exit surveys on social networks. The resumption of the permanent session is scheduled for 5:30 pm, to know the situation of the incidents and the closing of boxes. Read also: Burn urn in Tlaxcala; ITE says the day is quiet afcl/nv\n",
      "----------------\n",
      "BART Summary:\n",
      "The state and federal electoral authorities in Zacatecas confirmed that the basic box 667 could not be installed in the community of Guadalupe Victoria due to threats from cell officials. The electoral packages of this cell will be returned intact to the facilities of the district and municipal councils. The INE authorities had previously relocated several squares in the municipalities of Jerez, Tepetongo, and Monte Escobedo due to security issues. The resumption of the permanent session is scheduled for 5:30 pm to know the situation of the incidents and the closing of boxes.\n",
      "----------------\n",
      "StableBeluga Summary:\n",
      "The state and federal electoral authorities in Zacatecas confirmed that the basic box 667 could not be installed in the community of Guadalupe Victoria due to threats against cell officials. The electoral packages of this cell will be returned to the facilities of the district and municipal councils. The INE and IEEZ have relocated several squares in the municipalities of Jerez, Tepetongo, and Monte Escobedo due to the presence of organized crime groups. The SIJE reported 2,512 cells installed in the state, with 64 missing due to intermittent reports.\n"
     ]
    }
   ],
   "source": [
    "ix = np.random.randint(0,len(validation_set))\n",
    "art = validation_set[ix]['article']\n",
    "sb_summ = validation_set[ix]['summary']\n",
    "summary = summarizer.summarize(art)\n",
    "\n",
    "print(\"News Article:\")\n",
    "print(art)\n",
    "print('----------------')\n",
    "print(\"BART Summary:\")\n",
    "print(summary)\n",
    "print('----------------')\n",
    "print(\"StableBeluga Summary:\")\n",
    "print(sb_summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01bf5b92-4ede-4431-a0fe-325a8f0b26ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress_bar(iteration, total, bar_length=50):\n",
    "    progress = float(iteration) / float(total)\n",
    "    arrow = '=' * int(round(progress * bar_length) - 1)\n",
    "    spaces = ' ' * (bar_length - len(arrow))\n",
    "\n",
    "    print(f'Progress: [{arrow + spaces}] {int(progress * 100)}%', end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d089559b-6daa-4bd0-9ece-2785c19c0ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df = pd.DataFrame.from_dict(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc8302ec-4b07-4210-8cdd-ddce89580e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [================================================= ] 99%\r"
     ]
    }
   ],
   "source": [
    "bart_summaries = []\n",
    "for i, entry in enumerate(validation_set):\n",
    "    print_progress_bar(iteration=i, total=len(validation_set))\n",
    "    summary = summarizer.summarize(entry['article'])\n",
    "    bart_summaries.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e302984-82ba-4252-a4df-274315c304a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df['BART_summaries'] = bart_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe2acacd-72bb-4eef-8291-62a73652ae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df.to_json('datasets/bart_summaries_val_set.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f617d7-fb4a-43a8-8c96-a584b5789b3e",
   "metadata": {},
   "source": [
    "## Compare summaries by ROGUE metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30764692-497d-4f6a-b87d-1b05fea28151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b919592d-ac07-45c5-b595-327d689343a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46aef475-e3c8-4bc6-958c-95ad3bbf1f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSTRUCT MODEL ROGUE METRIC:\n",
      "{'rouge1': 0.6629992148125218, 'rouge2': 0.46701095620010635, 'rougeL': 0.5513249818595796, 'rougeLsum': 0.5515254233129148}\n"
     ]
    }
   ],
   "source": [
    "instruct_model_results = rouge.compute(\n",
    "    predictions=bart_summaries,\n",
    "    references=validation_df['summary'],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('INSTRUCT MODEL ROGUE METRIC:')\n",
    "print(instruct_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995c3a06-8cf0-4390-a761-c55c586be795",
   "metadata": {},
   "source": [
    "## Compare summaries using Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76436081-f242-4390-af49-a5167b96ece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model = summarizer.embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e5e78b6-f4ab-4f6a-a341-374d9a6ebaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs1 = emb_model.encode(validation_df['summary'])\n",
    "embs2 = emb_model.encode(validation_df['BART_summaries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7394f10a-1846-4243-beef-365706871acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity\n",
    "sims_mat = cosine_similarity(embs1, embs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d14e6a94-2af7-4404-b46e-5df0cbf6600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = np.diag(sims_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89ff2af3-049a-4a8c-b535-b1c6241da8f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    44277.000000\n",
       "mean         0.889844\n",
       "std          0.080382\n",
       "min          0.022816\n",
       "25%          0.858303\n",
       "50%          0.906960\n",
       "75%          0.942281\n",
       "max          1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(similarities).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c395e0b9-fe0e-4c22-849d-b14b97246e68",
   "metadata": {},
   "source": [
    "The model, in average, is producing an 88% of similar summaries. On the statistics, from the minimum and the 50% statistics, we can see that there might be outliers that are dragging the average down to 88%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5464dbf6-2b3b-43fb-971b-0c7678df086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df['summary_similarity']=similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ba58f9e-5d2e-4c98-b806-1d898a9a84f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>summary</th>\n",
       "      <th>BART_summaries</th>\n",
       "      <th>summary_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td></td>\n",
       "      <td>I will summarize the text for you. Please prov...</td>\n",
       "      <td></td>\n",
       "      <td>0.060218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td></td>\n",
       "      <td>I will summarize the text for you. Please prov...</td>\n",
       "      <td></td>\n",
       "      <td>0.060218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928</th>\n",
       "      <td></td>\n",
       "      <td>I will summarize the text for you. Please prov...</td>\n",
       "      <td></td>\n",
       "      <td>0.060218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2786</th>\n",
       "      <td>cvtp</td>\n",
       "      <td>I cannot provide a summary without more contex...</td>\n",
       "      <td></td>\n",
       "      <td>0.039395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3176</th>\n",
       "      <td></td>\n",
       "      <td>I will summarize the text for you. Please prov...</td>\n",
       "      <td></td>\n",
       "      <td>0.060218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42195</th>\n",
       "      <td></td>\n",
       "      <td>I will summarize the text for you. Please prov...</td>\n",
       "      <td></td>\n",
       "      <td>0.060218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43166</th>\n",
       "      <td></td>\n",
       "      <td>I will summarize the text for you. Please prov...</td>\n",
       "      <td></td>\n",
       "      <td>0.060218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43671</th>\n",
       "      <td></td>\n",
       "      <td>I will summarize the text for you. Please prov...</td>\n",
       "      <td></td>\n",
       "      <td>0.060218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43918</th>\n",
       "      <td></td>\n",
       "      <td>I will summarize the text for you. Please prov...</td>\n",
       "      <td></td>\n",
       "      <td>0.060218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44112</th>\n",
       "      <td></td>\n",
       "      <td>I will summarize the text for you. Please prov...</td>\n",
       "      <td></td>\n",
       "      <td>0.060218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      article                                            summary  \\\n",
       "80             I will summarize the text for you. Please prov...   \n",
       "691            I will summarize the text for you. Please prov...   \n",
       "1928           I will summarize the text for you. Please prov...   \n",
       "2786     cvtp  I cannot provide a summary without more contex...   \n",
       "3176           I will summarize the text for you. Please prov...   \n",
       "...       ...                                                ...   \n",
       "42195          I will summarize the text for you. Please prov...   \n",
       "43166          I will summarize the text for you. Please prov...   \n",
       "43671          I will summarize the text for you. Please prov...   \n",
       "43918          I will summarize the text for you. Please prov...   \n",
       "44112          I will summarize the text for you. Please prov...   \n",
       "\n",
       "      BART_summaries  summary_similarity  \n",
       "80                              0.060218  \n",
       "691                             0.060218  \n",
       "1928                            0.060218  \n",
       "2786                            0.039395  \n",
       "3176                            0.060218  \n",
       "...              ...                 ...  \n",
       "42195                           0.060218  \n",
       "43166                           0.060218  \n",
       "43671                           0.060218  \n",
       "43918                           0.060218  \n",
       "44112                           0.060218  \n",
       "\n",
       "[83 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outliers\n",
    "validation_df[validation_df['summary_similarity']<.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cf1609-bdee-417c-ba42-22ecb23d86a0",
   "metadata": {},
   "source": [
    "Outliers are comprised of empty article texts, which produce empty BART summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47e2c64b-473a-4850-9281-10e0b79fd19f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>summary</th>\n",
       "      <th>BART_summaries</th>\n",
       "      <th>summary_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19938</th>\n",
       "      <td>They have earned a place in the taste of the p...</td>\n",
       "      <td>.</td>\n",
       "      <td>Mexican celebrities have achieved success in v...</td>\n",
       "      <td>0.036312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26570</th>\n",
       "      <td>jlcg</td>\n",
       "      <td>I cannot summarize this as it appears to be a ...</td>\n",
       "      <td>Jjlcg:</td>\n",
       "      <td>0.065454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35191</th>\n",
       "      <td>The National Weather Service (SMN) pointed out...</td>\n",
       "      <td>.</td>\n",
       "      <td>The National Weather Service (SMN) predicts th...</td>\n",
       "      <td>0.022816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 article  \\\n",
       "19938  They have earned a place in the taste of the p...   \n",
       "26570                                               jlcg   \n",
       "35191  The National Weather Service (SMN) pointed out...   \n",
       "\n",
       "                                                 summary  \\\n",
       "19938                                                  .   \n",
       "26570  I cannot summarize this as it appears to be a ...   \n",
       "35191                                                  .   \n",
       "\n",
       "                                          BART_summaries  summary_similarity  \n",
       "19938  Mexican celebrities have achieved success in v...            0.036312  \n",
       "26570                                             Jjlcg:            0.065454  \n",
       "35191  The National Weather Service (SMN) predicts th...            0.022816  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df[\n",
    "    (validation_df['BART_summaries']!='')&\n",
    "    (validation_df['summary_similarity']<.1)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a13d89-fb97-4adb-b0ff-acedb3419ca5",
   "metadata": {},
   "source": [
    "We can see that outliers are present, due to article texts that are comprised of empty or incoherent text, as well as baseline summaries produced by the LLM that are not good summaries. We can further refine the model by removing this noisy observations from the train data. \n",
    "\n",
    "For the purpose of this excercise, we can use the 50% statistic, which is `.90`, as a similarity comparison between the summaries that our lightweigt BART model produces, against the summaries produced by the `StableBeluga-7B` LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1c934b-ee7a-428c-8b89-6a3b32aedf07",
   "metadata": {},
   "source": [
    "# Compare the time it takes for each model to perform the Summarization task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d76738d8-0b34-4809-a6a0-4289b6d58fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "for entry in validation_set[:500]:\n",
    "    start = time()\n",
    "    summary = summarizer.summarize(entry['article'])\n",
    "    end = time()\n",
    "    secs = end-start\n",
    "    times.append(secs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a85025f-068f-4e1e-976a-9977b898d656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2406021947860717"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average time for summarization is 1.2 seconds. That is 3x faster that the 3/4 seconds that Stable Beluga takes to summarize\n",
    "np.mean(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7494b54-cf88-4384-9a6c-6ccd6b32ecf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d90d37d9-d8af-41b6-a87d-8592a3305c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1d13aced1e4dc9892f8f1e55b3cc37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"stabilityai/StableBeluga-7B\", \n",
    "    use_fast=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"stabilityai/StableBeluga-7B\", \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    #low_cpu_mem_usage=True, \n",
    "    #device_map=\"auto\"\n",
    ")#.to(\"cuda\")\n",
    "model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb2940a7-927d-4b62-a468-4553c3c6a2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYS_PROMPT = \"### System:\\nYou are StableBeluga, an AI that follows instructions extremely well. Help as much as you can. Remember, be safe, and don't do anything illegal.\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a383e0f-6fbf-4e57-92aa-913e4e7da344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_article(article_text, model, tokenizer, top_n_perc = .2):\n",
    "    start = time()\n",
    "    message = \"\"\"\n",
    "    \n",
    "    Please summarize this:\n",
    "    \n",
    "    {0}\n",
    "    \"\"\".format(article_text)\n",
    "    prompt = f\"{SYS_PROMPT}### User: {message}\\n\\n### Assistant:\\n\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, do_sample=False, top_p=0.95, top_k=0, max_new_tokens=250)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "    end = time()\n",
    "    #print(end-start)\n",
    "    return response, end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e0e3691-5d18-416f-880d-8eba7e296191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jordi\\~\\venvs\\summaries\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jordi\\~\\venvs\\summaries\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "for entry in validation_set[:500]:\n",
    "    summary, secs = summarize_article(entry['article'], model, tokenizer)\n",
    "    times.append(secs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dd42aff-48de-4b17-b47b-03028c76d9a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7542019882202147"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stable Beluga taked 3.75 secons in average, to perfom a summary\n",
    "np.mean(times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f912f8-699d-44e0-b7c5-1a686bed7722",
   "metadata": {},
   "source": [
    "The lightweigth BART model performs the summarization task 3x faster than the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462131e5-ebbe-44ed-a547-52b87885acb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
